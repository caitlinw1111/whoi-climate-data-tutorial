{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8148ec62-2b7c-4c05-9b65-b38f1f5c1d35",
   "metadata": {},
   "source": [
    "# Outline of script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b35aa8-7384-4906-9d77-f463ff0cebc2",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e13d2c-e10b-4b5b-8f75-7a70dc1be3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07517718-7cad-49ae-a03a-2e8d9d3f0910",
   "metadata": {},
   "source": [
    "# Define constants/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a0834-084d-487d-b868-d1728c067148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "RAD_PER_DEG = 2 * np.pi / 360  # radians per degree\n",
    "R = 6.371e6  # radius of earth in m\n",
    "M_PER_KM = 1000  # meters per km\n",
    "\n",
    "## Filepaths\n",
    "# on clidex\n",
    "# era_fp = \"/vortexfs1/share/clidex/data/reanalysis/20CR/prmsl/prmsl.mon.mean.nc\"\n",
    "lme_fp = \"/vortex/clidex/data/model/CESM/LME/atm/psl\"\n",
    "\n",
    "# on cmip5 server (note: LME only has single ensemble member at the given directory)\n",
    "era_fp = \"/mnt/cmip5-data/reanalysis/era.20c/sfc/msl/moda/msl.mon.mean.nc\"\n",
    "noaa_fp = \"/mnt/cmip5-data/reanalysis/noaa.cires.20crv2c/monolevel/prmsl/monthly/prmsl.mon.mean.nc\"\n",
    "# lme_fp = \"/mnt/cmip5-data/CMIP5/output1/NCAR/CCSM4/past1000/mon/atmos/Amon/r1i1p1/psl/psl_Amon_CCSM4_past1000_r1i1p1_085001-185012.nc\"\n",
    "\n",
    "## Set plotting defaults\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d2cae-bd42-4433-9666-2c9a86b8141d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2aa9b-746b-4c3f-8b62-fc87265b5829",
   "metadata": {},
   "source": [
    "## Datasets used:\n",
    "- ERA-20C\n",
    "- NOAAâ€“CIRES 20CR\n",
    "- HadSLP2\n",
    "- CESM1 LME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339f345-4d61-4958-94c7-2c97d5c180ee",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dacba5-dcd4-4bbe-b9a4-cd51256e2a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trim_to_azores(data):\n",
    "    \"\"\"helper function to trim data to Azores lon/lat range.\"\"\"\n",
    "\n",
    "    ## Azores lon/lat range\n",
    "    lon_range = [-65, 15]\n",
    "    lat_range = [15, 65]\n",
    "\n",
    "    return data.sel(longitude=slice(*lon_range), latitude=slice(*lat_range))\n",
    "\n",
    "\n",
    "def djf_avg(data):\n",
    "    \"\"\"function to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## subset data\n",
    "    data_seasonal_avg = data.resample(time=\"QS-DEC\").mean()\n",
    "\n",
    "    ## get annual average for DJF\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def djf_avg_alt(data):\n",
    "    \"\"\"alternative (and slightly more general) function\n",
    "    to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## get 3-month rolling average average for DJF\n",
    "    data_seasonal_avg = data_djf.resample({\"time\": \"3MS\"}, label=\"left\").mean()\n",
    "\n",
    "    ## subset for djf avg\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def convert_longitude(longitude):\n",
    "    \"\"\"move longitude from range [0,360) to (-180,180].\n",
    "    Function accepts and returns a numpy array representing longitude values\"\"\"\n",
    "\n",
    "    ## find indices of longitudes which will become negative\n",
    "    is_neg = longitude > 180\n",
    "\n",
    "    ## change values at these indices\n",
    "    longitude[is_neg] = longitude[is_neg] - 360\n",
    "\n",
    "    return longitude\n",
    "\n",
    "\n",
    "def update_longitude_coord(data):\n",
    "    \"\"\"move longitude for dataset or dataarray from [0,360)\n",
    "    to (-180, 180]. Function accepts xr.dataset/xr.dataarray\n",
    "    and returns object of same type\"\"\"\n",
    "\n",
    "    ## get updated longitude coordinate\n",
    "    updated_longitude = convert_longitude(data.longitude.values)\n",
    "\n",
    "    ## sort updated coordinate to be increasing\n",
    "    updated_longitude = np.sort(updated_longitude)\n",
    "\n",
    "    ## sort data (\"reindex\") according to update coordinate\n",
    "    data = data.reindex({\"longitude\": updated_longitude})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def standardize_lonlat(\n",
    "    data, rename_coords=False, reverse_latitude=False, update_longitude=False\n",
    "):\n",
    "    \"\"\"update lonlat coordinates to be consistent across datasets.\n",
    "    In particular, make sure:\n",
    "        - coordinates are called \"longitude\" and \"latitude\"\n",
    "        - latitude is increasing\n",
    "        - longitude is increasing and in the range (-180, 180].\n",
    "    Function takes in and returns xr.dataarray or xr.dataset\"\"\"\n",
    "\n",
    "    ## change coord names from \"lat\" and \"lon\" to\n",
    "    ## \"latitude\" and \"longitude\", respectively\n",
    "    if rename_coords:\n",
    "        data = data.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "    ## change longitude from range [0,360) to (-180, 180]\n",
    "    if update_longitude:\n",
    "        data = update_longitude_coord(data)\n",
    "\n",
    "    ## switch direction of latitude so that it's increasing\n",
    "    if reverse_latitude:\n",
    "        latitude_updated = data.latitude.values[::-1]\n",
    "        data = data.reindex({\"latitude\": latitude_updated})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_era():\n",
    "\n",
    "    ## load raw data\n",
    "    data = xr.open_dataset(era_fp)[\"msl\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(data, update_longitude=True, reverse_latitude=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_noaa():\n",
    "\n",
    "    ## load raw data\n",
    "    data = xr.open_dataset(noaa_fp)[\"prmsl\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(\n",
    "        data, rename_coords=True, update_longitude=True, reverse_latitude=True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_lme_member(member_id):\n",
    "    \"\"\"\n",
    "    Function loads data from single member of CESM last-millenium ensemble (LME).\n",
    "    Args:\n",
    "    - 'member_id' is integer in [1,13] specifying which ensemble member to load\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get names of two files for each ensemble member\n",
    "    prefix = f\"{lme_fp}/b.e11.BLMTRC5CN.f19_g16.{member_id:03d}.cam.h0.PSL\"\n",
    "    fname0 = f\"{prefix}.085001-184912.nc\"\n",
    "    fname1 = f\"{prefix}.185001-200512.nc\"\n",
    "\n",
    "    ## Load data\n",
    "    data = xr.open_mfdataset([fname0, fname1], chunks={\"time\": 2000})[\"PSL\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(data, rename_coords=True, update_longitude=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_lme(member_ids=np.arange(1, 14).astype(int)):\n",
    "    \"\"\"load multiple LME members. 'member_ids' is an\n",
    "    array-like object specifying which ensemble members to load. This\n",
    "    is a list of integers in the range [1,13]\"\"\"\n",
    "\n",
    "    ## Load all ensemble members in list\n",
    "    data = [load_lme_member(i) for i in tqdm(member_ids)]\n",
    "\n",
    "    ## convert list to xarray\n",
    "    data = xr.concat(data, dim=pd.Index(member_ids, name=\"ensemble_member\"))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def spatial_avg(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"get global average of a quantity over the sphere.\n",
    "    Data is xr.dataarray/xr.dataset with  a 'regular' lon/lat grid\n",
    "    (equal lon/lat spacing between all data points)\"\"\"\n",
    "\n",
    "    ## get latitude-dependent weights\n",
    "    # first, convert latitude from degrees to radians\n",
    "    # conversion factor = (2 pi radians)/(360 degrees)\n",
    "    latitude_radians = data.latitude * (2 * np.pi) / 360\n",
    "    cos_lat = np.cos(latitude_radians)\n",
    "\n",
    "    ## Next, trim data to specified range\n",
    "    data_trim = data.sel(longitude=slice(*lon_range), latitude=slice(*lat_range))\n",
    "    cos_lat_trim = cos_lat.sel(latitude=slice(*lat_range))\n",
    "\n",
    "    ## Next, compute weighted avg\n",
    "    data_weighted = data_trim.weighted(weights=cos_lat_trim)\n",
    "    data_avg = data_weighted.mean([\"latitude\", \"longitude\"])\n",
    "\n",
    "    return data_avg\n",
    "\n",
    "\n",
    "def spatial_int(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"compute spatial integral of a quantity on the sphere. For convenience,\n",
    "    assume regular grid (constant lat/lon)\"\"\"\n",
    "\n",
    "    ## Get latitude/longitude in radians.\n",
    "    ## denote (lon,lat) in radians as (theta, phi)\n",
    "    rad_per_deg = 2 * np.pi / 360\n",
    "    theta = data.longitude * RAD_PER_DEG\n",
    "    phi = data.latitude * RAD_PER_DEG\n",
    "\n",
    "    ## get differences for integration (assumes constant differences)\n",
    "    dtheta = theta[1] - theta[0]\n",
    "    dphi = phi[1] - phi[0]\n",
    "\n",
    "    ## broadcast to grid\n",
    "    dtheta = dtheta * xr.ones_like(theta)\n",
    "    dphi = dphi * xr.ones_like(phi)\n",
    "\n",
    "    ## Get area of patch\n",
    "    dA = R**2 * np.cos(phi) * dphi * dtheta\n",
    "\n",
    "    ## Integrate\n",
    "    data_int = (data * dA).sum([\"latitude\", \"longitude\"])\n",
    "\n",
    "    return data_int\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"year\"):\n",
    "    \"\"\"Get linear trend for an xr.dataarray along specified dimension\"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = data.polyfit(dim=dim, deg=1)[\"polyfit_coefficients\"]\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0d3ae-fdeb-461c-92af-066469610b69",
   "metadata": {},
   "source": [
    "## Do the actual data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc2a14-188c-4cac-96df-e1fcfea9a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Load\" data (but not into memory yet)\n",
    "# slp_lme = load_lme(member_ids=[1, 2])\n",
    "slp_lme = load_lme(member_ids=np.arange(1, 14))\n",
    "slp_noaa = load_noaa()\n",
    "slp_era = load_era()\n",
    "\n",
    "## Get DJF average\n",
    "slp_noaa = djf_avg(slp_noaa).compute()\n",
    "slp_era = djf_avg(slp_era).compute()\n",
    "slp_lme = djf_avg(slp_lme).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26f044-848b-484b-a782-39003f27bb93",
   "metadata": {},
   "source": [
    "# Compute AHA metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1f1cc-1712-4d7b-a4fd-3da50b3f4cb0",
   "metadata": {},
   "source": [
    "From Cresswell-Clay et al. (2022): \"The AHA was defined as the area (km2) over the North Atlantic and Western Europe that had mean winter (Decemberâ€“Januaryâ€“February) SLP greater than 0.5 s.d. from the mean of the spatio-temporal winter SLP distribution (Fig. 1b). The region considered when calculating the AHA is bounded by the 60Â° W and 10Â° E meridians as well as the 10Â° N and 52Â° N latitudes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a71ec-5841-4414-99b4-36716affe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AHA(slp, norm_type=\"global_mean\"):\n",
    "    \"\"\"compute Azores High Area index, similar to Cresswell-Clay et al. (2022).\n",
    "    Defined as: area of Azores region which has (normalized) SLP exceeding 0.5 std\n",
    "    of long term average. Normalized SLP is defined as local SLP minus\n",
    "    globally-averaged SLP.\n",
    "\n",
    "    Args:\n",
    "    - slp is gridded SLP data (at global scale)\n",
    "    - norm_type is one of {\"global_mean\", \"detrend\"} specifying\n",
    "        how to normalize the AHA index\n",
    "\n",
    "    Note: returns area in KM^2.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get SLP anomaly in Azores regions\n",
    "    slp_azores = trim_to_azores(slp)\n",
    "\n",
    "    ## get globally-averaged SLP\n",
    "    slp_global_avg = spatial_avg(slp)\n",
    "\n",
    "    ## get normalized anomaly (func of lon/lat)\n",
    "    if norm_type == \"global_mean\":\n",
    "        slp_azores_norm = slp_azores - slp_global_avg\n",
    "\n",
    "    elif norm_type == \"detrend\":\n",
    "        trend = get_trend(slp_global_avg)\n",
    "        slp_azores_norm = slp_azores - trend\n",
    "\n",
    "    else:\n",
    "        print(\"Error: specify valid normalization type.\")\n",
    "\n",
    "    ## Get standard deviation (func of lon/lat)\n",
    "    slp_azores_mean = slp_azores_norm.mean(\"year\")\n",
    "    slp_azores_std = slp_azores_norm.std(\"year\")\n",
    "\n",
    "    ## Get mask of grid cells exceeding 0.5 std threshold\n",
    "    threshold = slp_azores_mean + 0.5 * slp_azores_std\n",
    "    exceeds_thresh = slp_azores_norm > threshold\n",
    "\n",
    "    ## Sum area of lon/lat cells exceeding threshold\n",
    "    ## convert from True/False to 1.0/0.0 for integration\n",
    "    AHA = spatial_int(exceeds_thresh.astype(float))\n",
    "\n",
    "    ## convert from m^2 to km^2\n",
    "    m2_per_km2 = 1e6\n",
    "    AHA *= 1 / (M_PER_KM**2)\n",
    "\n",
    "    return AHA\n",
    "\n",
    "\n",
    "def count_extremes(AHA, cutoff_perc=90.0, window=25):\n",
    "    \"\"\"Get rolling count of Azores High extreme events.\n",
    "    Args:\n",
    "    - cutoff_perc is percentile value in range (0 and 100) used to define\n",
    "        'extreme' events\n",
    "    - window is an integer specifying how many years the rolling window is.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get threshold for extreme events\n",
    "    threshold = AHA.quantile(q=cutoff_perc / 100)\n",
    "\n",
    "    ## Get boolean array: True if AHA exceeds thresh\n",
    "    exceeds_thresh = AHA > threshold\n",
    "\n",
    "    ## Get rolling count\n",
    "    rolling_count = exceeds_thresh.rolling(dim={\"year\": window}, center=True).sum()\n",
    "\n",
    "    ## remove NaN values at beginning and end\n",
    "    nan_count = np.round((window - 1) / 2).astype(int)\n",
    "    rolling_count = rolling_count.isel(year=slice(nan_count, -nan_count))\n",
    "\n",
    "    return rolling_count\n",
    "\n",
    "\n",
    "def count_extremes_wrapper(slp, norm_type=\"detrend\"):\n",
    "    \"\"\"wrapper function which takes in SLP and computes # of Azores High extremes\"\"\"\n",
    "    return count_extremes(compute_AHA(slp, norm_type=norm_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f7e10-e422-461d-843a-5de9b744a1da",
   "metadata": {},
   "source": [
    "Look at SLP over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8048-7657-48f0-94e5-ef4ed9972c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute area-averaged SLP\n",
    "slp_azores = spatial_avg(trim_to_azores(slp_noaa))\n",
    "slp_global = spatial_avg(slp_noaa)\n",
    "\n",
    "## get linear trend for global\n",
    "trend_fit = get_trend(slp_global)\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(slp_noaa.year, 1e-2 * slp_azores, label=\"Azores\")\n",
    "ax.plot(slp_noaa.year, 1e-2 * slp_global, label=\"Global\")\n",
    "ax.plot(trend_fit.year, 1e-2 * trend_fit, label=\"Global trend\", c=\"k\", ls=\"--\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "plt.show()\n",
    "\n",
    "## Plot before/after normalizing\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(\n",
    "    slp_noaa.year,\n",
    "    1e-2 * (slp_azores - slp_global + slp_global.mean()),\n",
    "    label=\"remove global mean\",\n",
    ")\n",
    "ax.plot(\n",
    "    trend_fit.year,\n",
    "    1e-2 * (slp_azores - trend_fit + trend_fit.mean()),\n",
    "    label=\"remove trend\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32599842-f855-44a2-a44f-77a9d805a58f",
   "metadata": {},
   "source": [
    "Plot # of LME extremes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49112e69-e42b-4480-a6d7-6137107a3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify which type of normalization\n",
    "## one of {\"global_mean\",\"detrend\"}\n",
    "norm_type = \"global_mean\"\n",
    "\n",
    "## count extremes in reanalysis\n",
    "count_noaa = count_extremes_wrapper(slp_noaa, norm_type=norm_type)\n",
    "count_era = count_extremes_wrapper(slp_era, norm_type=norm_type)\n",
    "\n",
    "## count in historical component of LME\n",
    "slp_lme_hist = slp_lme.sel(year=slice(1850, None))\n",
    "count_lme = count_extremes_wrapper(slp_lme_hist, norm_type=norm_type)\n",
    "count_lme_mean = count_lme.mean(\"ensemble_member\")\n",
    "count_lme_min = count_lme.min(\"ensemble_member\")\n",
    "count_lme_max = count_lme.max(\"ensemble_member\")\n",
    "\n",
    "## make plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot reanalysis\n",
    "ax.plot(count_noaa.year, count_noaa, label=\"NOAA\", ls=\"--\")\n",
    "ax.plot(count_era.year, count_era, label=\"ERA\", ls=\":\")\n",
    "\n",
    "## plot LME mean and range\n",
    "count_lme_plot = ax.plot(count_lme.year, count_lme_mean, label=\"LME\", ls=\"-\")\n",
    "for bound in [count_lme_min, count_lme_max]:\n",
    "    ax.plot(bound.year, bound, c=count_lme_plot[0].get_color(), lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Count (25-yr rolling)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e15d4f-6f95-4cf6-8134-4fa3db9d87b9",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06f1a7-be80-4bd4-a15c-a0f832362f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trim to Azores region\n",
    "# slp_noaa_azores =\n",
    "\n",
    "# ## now subset in space and get DJF average (loads into memory)\n",
    "# print(\"Loading LME\")\n",
    "# slp_lme = reduce(slp_lme)\n",
    "\n",
    "# print(\"Loading NOAA\")\n",
    "# slp_noaa = reduce(slp_noaa)\n",
    "\n",
    "# print(\"Loading 20CR\")\n",
    "# slp_era = reduce(slp_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6f12b-3dff-4c19-94e7-fea0312a5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cos_lat = np.cos(slp_noaa.latitude * 2 * np.pi / 360)\n",
    "slp_noaa_weighted = slp_noaa.weighted(weights=cos_lat)\n",
    "slp_weighted = slp_noaa_weighted.mean([\"longitude\", \"latitude\"])\n",
    "slp_unweighted = slp_noaa.mean([\"longitude\", \"latitude\"])\n",
    "# (slp_noaa * cos_lat).sum(\"latitude\") / cos_lat.sum(\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15222b7e-d4e5-431e-84a3-e07a4e2e44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_azores = trim_to_azores(slp_noaa)\n",
    "slp_azores = slp_azores.weighted(np.cos(slp_azores.latitude * 2 * np.pi / 360))\n",
    "slp_azores = slp_azores.mean([\"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cccac-3922-404e-a708-049bb8a87052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "y0 = 1900\n",
    "scipy.stats.pearsonr(\n",
    "    slp_azores.sel(year=slice(y0, None)), slp_weighted.sel(year=slice(y0, None))\n",
    ")\n",
    "\n",
    "scipy.stats.pearsonr(\n",
    "    slp_azores.sel(year=slice(y0, None)), slp_unweighted.sel(year=slice(y0, None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c990-9a5e-4f24-89bb-7e43a133668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(slp_weighted.year, slp_weighted, label=\"weighted\")\n",
    "ax.plot(slp_weighted.year, spatial_avg(slp_noaa), label=\"weighted\", ls=\"--\", c=\"k\")\n",
    "ax.plot(slp_unweighted.year, slp_unweighted, label=\"unweighted\")\n",
    "ax.plot(slp_azores.year, slp_azores, label=\"azores\")\n",
    "ax.axhline(1.013e5, ls=\"--\", c=\"k\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(slp_azores.year, slp_azores, label=\"azores\")\n",
    "ax.plot(\n",
    "    slp_azores.year, slp_azores - slp_weighted + slp_weighted.mean(), label=\"azores\"\n",
    ")\n",
    "ax.axhline(1.013e5, ls=\"--\", c=\"k\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b14fad-a724-4a75-83c2-95b0f2e8ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy.stats.pearsonr(slp_azores / slp_weighted, slp_azores - slp_weighted))\n",
    "print(\n",
    "    scipy.stats.pearsonr(\n",
    "        np.log(slp_azores) - np.log(slp_weighted), slp_azores - slp_weighted\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e19d7-cca6-4ed5-b463-a694d862687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = lambda x: (x - x.mean()) / x.std()\n",
    "plt.plot(norm(slp_azores / slp_weighted))\n",
    "plt.plot(norm(slp_azores - slp_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a330e9-cced-4091-8a1e-1d2d72f4cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anom(data):\n",
    "    \"\"\"Compute anomalies relative to long-term mean\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def AHA(data_anom):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a64e26-ddba-45e0-be67-5648ff84c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_noaa.chunk({\"time\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6603f3-d94b-4507-8061-93207a451a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = slp_noaa.isel(latitude=10, longitude=10)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(djf_avg(x))\n",
    "ax.plot(djf_avg2(x))\n",
    "ax.set_xlim([-1, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ad98a-b0ef-4671-9e03-27d49ac788b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg2(slp_noaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c115dcd-c38e-4664-b752-10e12687210e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef31b92-0635-42d1-94a7-53df7ac3beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg2(slp_noaa).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2857f-467f-4b08-b1ad-35900ef91e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg(slp_noaa).year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff0ac0-89b1-4a5a-afc3-36b2feff3aaa",
   "metadata": {},
   "source": [
    "# Compute AHA index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d778e6c-87b4-483a-80e4-2ade90e2fa87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
