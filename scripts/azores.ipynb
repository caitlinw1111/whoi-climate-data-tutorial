{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8148ec62-2b7c-4c05-9b65-b38f1f5c1d35",
   "metadata": {},
   "source": [
    "# Outline of script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b35aa8-7384-4906-9d77-f463ff0cebc2",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e13d2c-e10b-4b5b-8f75-7a70dc1be3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as mticker\n",
    "import cmocean\n",
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07517718-7cad-49ae-a03a-2e8d9d3f0910",
   "metadata": {},
   "source": [
    "# Define constants/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a0834-084d-487d-b868-d1728c067148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants\n",
    "RAD_PER_DEG = 2 * np.pi / 360  # radians per degree\n",
    "R = 6.371e6  # radius of earth in m\n",
    "M_PER_KM = 1000  # meters per km\n",
    "\n",
    "## Filepaths\n",
    "# on clidex\n",
    "# era_fp = \"/vortexfs1/share/clidex/data/reanalysis/20CR/prmsl/prmsl.mon.mean.nc\"\n",
    "lme_fp = \"/vortex/clidex/data/model/CESM/LME/atm/psl\"\n",
    "\n",
    "# on cmip5 server (note: LME only has single ensemble member at the given directory)\n",
    "era_fp = \"/mnt/cmip5-data/reanalysis/era.20c/sfc/msl/moda/msl.mon.mean.nc\"\n",
    "noaa_fp = \"/mnt/cmip5-data/reanalysis/noaa.cires.20crv2c/monolevel/prmsl/monthly/prmsl.mon.mean.nc\"\n",
    "# lme_fp = \"/mnt/cmip5-data/CMIP5/output1/NCAR/CCSM4/past1000/mon/atmos/Amon/r1i1p1/psl/psl_Amon_CCSM4_past1000_r1i1p1_085001-185012.nc\"\n",
    "\n",
    "## Set plotting defaults\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d2cae-bd42-4433-9666-2c9a86b8141d",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2aa9b-746b-4c3f-8b62-fc87265b5829",
   "metadata": {},
   "source": [
    "## Datasets used:\n",
    "- ERA-20C\n",
    "- NOAAâ€“CIRES 20CR\n",
    "- HadSLP2\n",
    "- CESM1 LME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339f345-4d61-4958-94c7-2c97d5c180ee",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dacba5-dcd4-4bbe-b9a4-cd51256e2a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trim_to_north_atlantic(data):\n",
    "    \"\"\"convenience function to trim data north atlantic domain\"\"\"\n",
    "\n",
    "    return data.sel(longitude=slice(-70, 15), latitude=slice(0, 70))\n",
    "\n",
    "\n",
    "def trim_to_azores(data):\n",
    "    \"\"\"convenience function to trim data to Azores lon/lat range.\"\"\"\n",
    "\n",
    "    return data.sel(longitude=slice(-60, 10), latitude=slice(10, 52))\n",
    "\n",
    "\n",
    "def djf_avg(data):\n",
    "    \"\"\"function to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## subset data\n",
    "    data_seasonal_avg = data.resample(time=\"QS-DEC\").mean()\n",
    "\n",
    "    ## get annual average for DJF\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def djf_avg_alt(data):\n",
    "    \"\"\"alternative (and slightly more general) function\n",
    "    to trim data to Dec/Jan/Feb (DJF) months\"\"\"\n",
    "\n",
    "    ## get 3-month rolling average average for DJF\n",
    "    data_seasonal_avg = data_djf.resample({\"time\": \"3MS\"}, label=\"left\").mean()\n",
    "\n",
    "    ## subset for djf avg\n",
    "    is_djf = data_seasonal_avg.time.dt.month == 12\n",
    "    data_djf_avg = data_seasonal_avg.sel(time=is_djf)\n",
    "\n",
    "    ## drop 1st and last averages, bc they only have\n",
    "    ## 2 samples and 1 sample, respectively\n",
    "    data_djf_avg = data_djf_avg.isel(time=slice(1, -1))\n",
    "\n",
    "    ## Replace time index with year, corresponding to january.\n",
    "    ## '+1' is because 'time's year uses december\n",
    "    year = data_djf_avg.time.dt.year + 1\n",
    "    data_djf_avg[\"time\"] = year\n",
    "    data_djf_avg = data_djf_avg.rename({\"time\": \"year\"})\n",
    "\n",
    "    return data_djf_avg\n",
    "\n",
    "\n",
    "def convert_longitude(longitude):\n",
    "    \"\"\"move longitude from range [0,360) to (-180,180].\n",
    "    Function accepts and returns a numpy array representing longitude values\"\"\"\n",
    "\n",
    "    ## find indices of longitudes which will become negative\n",
    "    is_neg = longitude > 180\n",
    "\n",
    "    ## change values at these indices\n",
    "    longitude[is_neg] = longitude[is_neg] - 360\n",
    "\n",
    "    return longitude\n",
    "\n",
    "\n",
    "def update_longitude_coord(data):\n",
    "    \"\"\"move longitude for dataset or dataarray from [0,360)\n",
    "    to (-180, 180]. Function accepts xr.dataset/xr.dataarray\n",
    "    and returns object of same type\"\"\"\n",
    "\n",
    "    ## get updated longitude coordinate\n",
    "    updated_longitude = convert_longitude(data.longitude.values)\n",
    "\n",
    "    ## sort updated coordinate to be increasing\n",
    "    updated_longitude = np.sort(updated_longitude)\n",
    "\n",
    "    ## sort data (\"reindex\") according to update coordinate\n",
    "    data = data.reindex({\"longitude\": updated_longitude})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def standardize_lonlat(\n",
    "    data, rename_coords=False, reverse_latitude=False, update_longitude=False\n",
    "):\n",
    "    \"\"\"update lonlat coordinates to be consistent across datasets.\n",
    "    In particular, make sure:\n",
    "        - coordinates are called \"longitude\" and \"latitude\"\n",
    "        - latitude is increasing\n",
    "        - longitude is increasing and in the range (-180, 180].\n",
    "    Function takes in and returns xr.dataarray or xr.dataset\"\"\"\n",
    "\n",
    "    ## change coord names from \"lat\" and \"lon\" to\n",
    "    ## \"latitude\" and \"longitude\", respectively\n",
    "    if rename_coords:\n",
    "        data = data.rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "    ## change longitude from range [0,360) to (-180, 180]\n",
    "    if update_longitude:\n",
    "        data = update_longitude_coord(data)\n",
    "\n",
    "    ## switch direction of latitude so that it's increasing\n",
    "    if reverse_latitude:\n",
    "        latitude_updated = data.latitude.values[::-1]\n",
    "        data = data.reindex({\"latitude\": latitude_updated})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_era():\n",
    "\n",
    "    ## load raw data\n",
    "    data = xr.open_dataset(era_fp)[\"msl\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(data, update_longitude=True, reverse_latitude=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_noaa():\n",
    "\n",
    "    ## load raw data\n",
    "    data = xr.open_dataset(noaa_fp)[\"prmsl\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(\n",
    "        data, rename_coords=True, update_longitude=True, reverse_latitude=True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_lme_member(member_id):\n",
    "    \"\"\"\n",
    "    Function loads data from single member of CESM last-millenium ensemble (LME).\n",
    "    Args:\n",
    "    - 'member_id' is integer in [1,13] specifying which ensemble member to load\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get names of two files for each ensemble member\n",
    "    prefix = f\"{lme_fp}/b.e11.BLMTRC5CN.f19_g16.{member_id:03d}.cam.h0.PSL\"\n",
    "    fname0 = f\"{prefix}.085001-184912.nc\"\n",
    "    fname1 = f\"{prefix}.185001-200512.nc\"\n",
    "\n",
    "    ## Load data\n",
    "    data = xr.open_mfdataset([fname0, fname1], chunks={\"time\": 2000})[\"PSL\"]\n",
    "\n",
    "    ## update coordinates\n",
    "    data = standardize_lonlat(data, rename_coords=True, update_longitude=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_lme(member_ids=np.arange(1, 14).astype(int)):\n",
    "    \"\"\"load multiple LME members. 'member_ids' is an\n",
    "    array-like object specifying which ensemble members to load. This\n",
    "    is a list of integers in the range [1,13]\"\"\"\n",
    "\n",
    "    ## Load all ensemble members in list\n",
    "    data = [load_lme_member(i) for i in tqdm(member_ids)]\n",
    "\n",
    "    ## convert list to xarray\n",
    "    data = xr.concat(data, dim=pd.Index(member_ids, name=\"ensemble_member\"))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def spatial_avg(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"get global average of a quantity over the sphere.\n",
    "    Data is xr.dataarray/xr.dataset with  a 'regular' lon/lat grid\n",
    "    (equal lon/lat spacing between all data points)\"\"\"\n",
    "\n",
    "    ## get latitude-dependent weights\n",
    "    # first, convert latitude from degrees to radians\n",
    "    # conversion factor = (2 pi radians)/(360 degrees)\n",
    "    latitude_radians = data.latitude * (2 * np.pi) / 360\n",
    "    cos_lat = np.cos(latitude_radians)\n",
    "\n",
    "    ## Next, trim data to specified range\n",
    "    data_trim = data.sel(longitude=slice(*lon_range), latitude=slice(*lat_range))\n",
    "    cos_lat_trim = cos_lat.sel(latitude=slice(*lat_range))\n",
    "\n",
    "    ## Next, compute weighted avg\n",
    "    data_weighted = data_trim.weighted(weights=cos_lat_trim)\n",
    "    data_avg = data_weighted.mean([\"latitude\", \"longitude\"])\n",
    "\n",
    "    return data_avg\n",
    "\n",
    "\n",
    "def spatial_int(data, lon_range=[None, None], lat_range=[None, None]):\n",
    "    \"\"\"compute spatial integral of a quantity on the sphere. For convenience,\n",
    "    assume regular grid (constant lat/lon)\"\"\"\n",
    "\n",
    "    ## Get latitude/longitude in radians.\n",
    "    ## denote (lon,lat) in radians as (theta, phi)\n",
    "    rad_per_deg = 2 * np.pi / 360\n",
    "    theta = data.longitude * RAD_PER_DEG\n",
    "    phi = data.latitude * RAD_PER_DEG\n",
    "\n",
    "    ## get differences for integration (assumes constant differences)\n",
    "    dtheta = theta[1] - theta[0]\n",
    "    dphi = phi[1] - phi[0]\n",
    "\n",
    "    ## broadcast to grid\n",
    "    dtheta = dtheta * xr.ones_like(theta)\n",
    "    dphi = dphi * xr.ones_like(phi)\n",
    "\n",
    "    ## Get area of patch\n",
    "    dA = R**2 * np.cos(phi) * dphi * dtheta\n",
    "\n",
    "    ## Integrate\n",
    "    data_int = (data * dA).sum([\"latitude\", \"longitude\"])\n",
    "\n",
    "    return data_int\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"year\"):\n",
    "    \"\"\"Get linear trend for an xr.dataarray along specified dimension\"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = data.polyfit(dim=dim, deg=1)[\"polyfit_coefficients\"]\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0d3ae-fdeb-461c-92af-066469610b69",
   "metadata": {},
   "source": [
    "## Do the actual data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc2a14-188c-4cac-96df-e1fcfea9a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Load\" data (but not into memory yet)\n",
    "# slp_lme = load_lme(member_ids=[1, 2])\n",
    "# slp_lme = load_lme(member_ids=np.arange(1, 14))\n",
    "slp_noaa = load_noaa()\n",
    "slp_era = load_era()\n",
    "\n",
    "## Get DJF average\n",
    "slp_noaa = djf_avg(slp_noaa).compute()\n",
    "slp_era = djf_avg(slp_era).compute()\n",
    "# slp_lme = djf_avg(slp_lme).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26f044-848b-484b-a782-39003f27bb93",
   "metadata": {},
   "source": [
    "# Compute AHA metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb1f1cc-1712-4d7b-a4fd-3da50b3f4cb0",
   "metadata": {},
   "source": [
    "From Cresswell-Clay et al. (2022): \"The AHA was defined as the area (km2) over the North Atlantic and Western Europe that had mean winter (Decemberâ€“Januaryâ€“February) SLP greater than 0.5 s.d. from the mean of the spatio-temporal winter SLP distribution (Fig. 1b). The region considered when calculating the AHA is bounded by the 60Â° W and 10Â° E meridians as well as the 10Â° N and 52Â° N latitudes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a71ec-5841-4414-99b4-36716affe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AHA(slp, norm_type=\"global_mean\"):\n",
    "    \"\"\"compute Azores High Area index, similar to Cresswell-Clay et al. (2022).\n",
    "    Defined as: area of Azores region which has (normalized) SLP exceeding 0.5 std\n",
    "    of long term average. Normalized SLP is defined as local SLP minus\n",
    "    globally-averaged SLP.\n",
    "\n",
    "    Args:\n",
    "    - slp is gridded SLP data (at global scale)\n",
    "    - norm_type is one of {\"global_mean\", \"detrend\"} specifying\n",
    "        how to normalize the AHA index\n",
    "\n",
    "    Note: returns area in KM^2.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get SLP anomaly in Azores regions\n",
    "    slp_azores = trim_to_azores(slp)\n",
    "\n",
    "    ## get globally-averaged SLP\n",
    "    slp_global_avg = spatial_avg(slp)\n",
    "\n",
    "    ## get normalized anomaly (func of lon/lat)\n",
    "    if norm_type == \"global_mean\":\n",
    "        slp_azores_norm = slp_azores - slp_global_avg\n",
    "\n",
    "    elif norm_type == \"detrend\":\n",
    "        trend = get_trend(slp_global_avg)\n",
    "        slp_azores_norm = slp_azores - trend\n",
    "\n",
    "    else:\n",
    "        print(\"Error: specify valid normalization type.\")\n",
    "\n",
    "    ## Get standard deviation (func of lon/lat)\n",
    "    slp_azores_mean = slp_azores_norm.mean(\"year\")\n",
    "    slp_azores_std = slp_azores_norm.std(\"year\")\n",
    "\n",
    "    ## Get mask of grid cells exceeding 0.5 std threshold\n",
    "    threshold = slp_azores_mean + 0.5 * slp_azores_std\n",
    "    exceeds_thresh = slp_azores_norm > threshold\n",
    "\n",
    "    ## Sum area of lon/lat cells exceeding threshold\n",
    "    ## convert from True/False to 1.0/0.0 for integration\n",
    "    AHA = spatial_int(exceeds_thresh.astype(float))\n",
    "\n",
    "    ## convert from m^2 to km^2\n",
    "    m2_per_km2 = 1e6\n",
    "    AHA *= 1 / (M_PER_KM**2)\n",
    "\n",
    "    return AHA\n",
    "\n",
    "\n",
    "def count_extremes(AHA, cutoff_perc=90.0, window=25):\n",
    "    \"\"\"Get rolling count of Azores High extreme events.\n",
    "    Args:\n",
    "    - cutoff_perc is percentile value in range (0 and 100) used to define\n",
    "        'extreme' events\n",
    "    - window is an integer specifying how many years the rolling window is.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get threshold for extreme events\n",
    "    threshold = AHA.quantile(q=cutoff_perc / 100)\n",
    "\n",
    "    ## Get boolean array: True if AHA exceeds thresh\n",
    "    exceeds_thresh = AHA > threshold\n",
    "\n",
    "    ## Get rolling count\n",
    "    rolling_count = exceeds_thresh.rolling(dim={\"year\": window}, center=True).sum()\n",
    "\n",
    "    ## remove NaN values at beginning and end\n",
    "    nan_count = np.round((window - 1) / 2).astype(int)\n",
    "    rolling_count = rolling_count.isel(year=slice(nan_count, -nan_count))\n",
    "\n",
    "    return rolling_count\n",
    "\n",
    "\n",
    "def count_extremes_wrapper(slp, norm_type=\"detrend\"):\n",
    "    \"\"\"wrapper function which takes in SLP and computes # of Azores High extremes\"\"\"\n",
    "    return count_extremes(compute_AHA(slp, norm_type=norm_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f7e10-e422-461d-843a-5de9b744a1da",
   "metadata": {},
   "source": [
    "Look at SLP over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a8048-7657-48f0-94e5-ef4ed9972c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute area-averaged SLP\n",
    "slp_azores = spatial_avg(trim_to_azores(slp_noaa))\n",
    "slp_global = spatial_avg(slp_noaa)\n",
    "\n",
    "## get linear trend for global\n",
    "trend_fit = get_trend(slp_global)\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(slp_noaa.year, 1e-2 * slp_azores, label=\"Azores\")\n",
    "ax.plot(slp_noaa.year, 1e-2 * slp_global, label=\"Global\")\n",
    "ax.plot(trend_fit.year, 1e-2 * trend_fit, label=\"Global trend\", c=\"k\", ls=\"--\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "plt.show()\n",
    "\n",
    "## Plot before/after normalizing\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(\n",
    "    slp_noaa.year,\n",
    "    1e-2 * (slp_azores - slp_global + slp_global.mean()),\n",
    "    label=\"remove global mean\",\n",
    ")\n",
    "ax.plot(\n",
    "    trend_fit.year,\n",
    "    1e-2 * (slp_azores - trend_fit + trend_fit.mean()),\n",
    "    label=\"remove trend\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"SLP (hPa)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32599842-f855-44a2-a44f-77a9d805a58f",
   "metadata": {},
   "source": [
    "#### Fig 2c: # of AHA extremes since ~1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49112e69-e42b-4480-a6d7-6137107a3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count extremes in reanalysis\n",
    "count_noaa = count_extremes_wrapper(slp_noaa, norm_type=\"global_mean\")\n",
    "count_era = count_extremes_wrapper(slp_era, norm_type=\"global_mean\")\n",
    "\n",
    "## count in historical component of LME\n",
    "slp_lme_hist = slp_lme.sel(year=slice(1850, None))\n",
    "count_lme = count_extremes_wrapper(slp_lme_hist, norm_type=\"global_mean\")\n",
    "count_lme_mean = count_lme.mean(\"ensemble_member\")\n",
    "count_lme_min = count_lme.min(\"ensemble_member\")\n",
    "count_lme_max = count_lme.max(\"ensemble_member\")\n",
    "\n",
    "## make plot\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot reanalysis\n",
    "ax.plot(count_noaa.year, count_noaa, label=\"NOAA\", ls=\"--\")\n",
    "ax.plot(count_era.year, count_era, label=\"ERA\", ls=\":\")\n",
    "\n",
    "## plot LME mean and range\n",
    "count_lme_plot = ax.plot(count_lme.year, count_lme_mean, label=\"LME\", ls=\"-\")\n",
    "for bound in [count_lme_min, count_lme_max]:\n",
    "    ax.plot(bound.year, bound, c=count_lme_plot[0].get_color(), lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Count (25-yr rolling)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67501502-1023-470e-ae80-135a80c4b67d",
   "metadata": {},
   "source": [
    "# Spatial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad0647-8838-4178-8ee9-c2ffbb184385",
   "metadata": {},
   "source": [
    "#### Functions to compute and plot composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe5754-4964-4e37-b6b3-51609b317fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_composite(data, year_range0=[1950, 1979], year_range1=[1980, 2007]):\n",
    "    \"\"\"get composite, defined as difference between data\n",
    "    when averaged over year_range1 and year_range0.\n",
    "    \"\"\"\n",
    "\n",
    "    ## compute means\n",
    "    mean1 = data.sel(year=slice(*year_range1)).mean(\"year\")\n",
    "    mean0 = data.sel(year=slice(*year_range0)).mean(\"year\")\n",
    "\n",
    "    return mean1 - mean0\n",
    "\n",
    "\n",
    "def plot_setup_helper(ax, scale=1):\n",
    "    \"\"\"Create map background for plotting spatial data.\n",
    "    Returns modified 'ax' object.\"\"\"\n",
    "\n",
    "    ## specify range and ticklabels for plot\n",
    "    lon_range = [-70, 10]\n",
    "    lat_range = [3, 70]\n",
    "    xticks = [-60, -40, -20, 0]\n",
    "    yticks = [20, 40, 60]\n",
    "\n",
    "    ## specify transparency/linewidths\n",
    "    grid_alpha = 0.0 * scale\n",
    "    grid_linewidth = 0.5 * scale\n",
    "    coastline_linewidth = 0.3 * scale\n",
    "    label_size = 8 * scale\n",
    "\n",
    "    ## crop map and plot coastlines\n",
    "    ax.set_extent([*lon_range, *lat_range], crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(linewidth=coastline_linewidth)\n",
    "\n",
    "    ## plot grid\n",
    "    gl = ax.gridlines(\n",
    "        draw_labels=True,\n",
    "        linestyle=\"--\",\n",
    "        alpha=grid_alpha,\n",
    "        linewidth=grid_linewidth,\n",
    "        color=\"k\",\n",
    "        zorder=1.05,\n",
    "    )\n",
    "\n",
    "    ## add tick labels\n",
    "    gl.bottom_labels = False\n",
    "    gl.right_labels = False\n",
    "    gl.xlabel_style = {\"size\": label_size}\n",
    "    gl.ylabel_style = {\"size\": label_size}\n",
    "    gl.ylocator = mticker.FixedLocator(yticks)\n",
    "    gl.xlocator = mticker.FixedLocator(xticks)\n",
    "\n",
    "    return ax, gl\n",
    "\n",
    "\n",
    "def make_cb_range(amp, delta):\n",
    "    \"\"\"Make colorbar_range for cmo.balance\"\"\"\n",
    "    return np.concatenate(\n",
    "        [np.arange(-amp, 0, delta), np.arange(delta, amp + delta, delta)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88f06a-5c78-4c46-8d92-0bbb02e6db60",
   "metadata": {},
   "source": [
    "#### Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2341830-2be0-4b75-9f24-7a605e81e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_noaa_uv10():\n",
    "    \"\"\"Load NOAA U10 and V10 data (eastward and northward windspeed at 10m)\"\"\"\n",
    "\n",
    "    ## load raw files for u and v\n",
    "    fp = \"/mnt/cmip5-data/reanalysis/noaa.cires.20crv2c/gaussian\"\n",
    "    u10_noaa = xr.open_dataset(f\"{fp}/uwnd.10m/monthly/uwnd.10m.mon.mean.nc\")\n",
    "    v10_noaa = xr.open_dataset(f\"{fp}/vwnd.10m/monthly/vwnd.10m.mon.mean.nc\")\n",
    "\n",
    "    ## merge into single dataset\n",
    "    uv10_noaa = xr.merge([u10_noaa, v10_noaa])\n",
    "\n",
    "    ## update coordinates\n",
    "    uv10_noaa = standardize_lonlat(\n",
    "        uv10_noaa, rename_coords=True, update_longitude=True, reverse_latitude=True\n",
    "    )\n",
    "\n",
    "    ## trim to north atlantic\n",
    "    uv10_noaa = trim_to_north_atlantic(uv10_noaa).compute()\n",
    "\n",
    "    return uv10_noaa\n",
    "\n",
    "\n",
    "def load_noaa_precip():\n",
    "    \"\"\"Load NOAA precipitation data from CMIP server.\n",
    "    Returns xr.dataarray with units of mm\"\"\"\n",
    "\n",
    "    ## load in precipitation rate (units: kg/m^2/s)\n",
    "    fp = \"/mnt/cmip5-data/reanalysis/noaa.cires.20crv2c/gaussian/prate/monthly/prate.mon.mean.nc\"\n",
    "    precip_rate = xr.open_dataset(fp)[\"prate\"]\n",
    "\n",
    "    ## fix lonlat coordinates\n",
    "    precip_rate = standardize_lonlat(\n",
    "        precip_rate, rename_coords=True, update_longitude=True, reverse_latitude=True\n",
    "    )\n",
    "\n",
    "    ## trim to North Atlantic domain\n",
    "    precip_rate = trim_to_north_atlantic(precip_rate).compute()\n",
    "\n",
    "    ## Convert units from mass/area to height by dividing\n",
    "    ## by density of liquid water:\n",
    "    ## (km m^-2 s^-1) * (kg^-1 m^3) = m s^-1\n",
    "    density_water = 1000  # units: kg m^-3\n",
    "    precip_rate = precip_rate / density_water  # new units of m/s\n",
    "\n",
    "    ## convert from month-averaged rate (units: m/s) to\n",
    "    ## month total (units: m), by multiplyin by no. of seconds\n",
    "    ## in each month. Units: (m s^-1) * s = m.\n",
    "    ## Note: days_per_month depends on month (so is\n",
    "    ## array, not a scalar, in the code below).\n",
    "    days_per_month = precip_rate.time.dt.days_in_month\n",
    "    seconds_per_day = 86400\n",
    "    seconds_per_month = seconds_per_day * days_per_month\n",
    "    precip = precip_rate * seconds_per_month\n",
    "\n",
    "    ## convert units from m to mm\n",
    "    mm_per_m = 1000\n",
    "    precip = precip * mm_per_m\n",
    "\n",
    "    return precip.rename(\"precip\")\n",
    "\n",
    "\n",
    "def load_noaa_lsm():\n",
    "    \"\"\"Load NOAA land-sea mask for NOAA reanalysis\"\"\"\n",
    "\n",
    "    ## load raw data\n",
    "    url = \"http://psl.noaa.gov/thredds/dodsC/Datasets/20thC_ReanV2c/gaussian/time_invariant/land.nc\"\n",
    "    lsm = xr.open_dataset(url)[\"land\"].isel(time=0, drop=True).compute()\n",
    "\n",
    "    ## fix coordinates to match other datasets\n",
    "    lsm = standardize_lonlat(\n",
    "        lsm, rename_coords=True, update_longitude=True, reverse_latitude=True\n",
    "    )\n",
    "\n",
    "    ## trim to north atlantic\n",
    "    lsm = trim_to_north_atlantic(lsm)\n",
    "\n",
    "    return lsm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130a5cf-061b-40df-a2d0-325c8677d99e",
   "metadata": {},
   "source": [
    "#### Do the data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c2b546-746f-4eea-b4f4-ab1bff6a48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load u/v data\n",
    "uv10_noaa = load_noaa_uv10()\n",
    "uv10_noaa = djf_avg(uv10_noaa)\n",
    "\n",
    "## get land-sea mask\n",
    "lsm_noaa = load_noaa_lsm()\n",
    "\n",
    "## load precip data, get DJF average\n",
    "precip_noaa = load_noaa_precip()\n",
    "precip_noaa = djf_avg(precip_noaa)\n",
    "\n",
    "## Regrid SLP to match precip and uv10;\n",
    "## this will make compositing easier\n",
    "regridder = xe.Regridder(ds_in=slp_noaa, ds_out=lsm_noaa, method=\"bilinear\")\n",
    "slp_noaa_regrid = regridder(slp_noaa).rename(\"slp\")\n",
    "\n",
    "## merge datasets\n",
    "data_noaa = xr.merge([uv10_noaa, precip_noaa, slp_noaa_regrid])\n",
    "\n",
    "## compute composite and climatology\n",
    "comp = make_composite(data_noaa)\n",
    "clim = data_noaa.sel(year=slice(1950, 2007)).mean(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8c040-f93b-4120-a527-aa4dbdfe56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create figure\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "## add first plotting background\n",
    "ax0 = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax0, gl0 = plot_setup_helper(ax0, scale=1.2)\n",
    "\n",
    "## plot precip in colors over land:\n",
    "precip_plot = ax0.contourf(\n",
    "    comp.longitude,\n",
    "    comp.latitude,\n",
    "    comp[\"precip\"],\n",
    "    cmap=\"cmo.diff_r\",\n",
    "    levels=make_cb_range(35, 3.5),\n",
    "    extend=\"both\",\n",
    ")\n",
    "\n",
    "## plot composite in colors\n",
    "ax0.contourf(\n",
    "    comp.longitude,\n",
    "    comp.latitude,\n",
    "    comp[\"slp\"].where(lsm_noaa < 1),\n",
    "    cmap=\"cmo.balance\",\n",
    "    levels=make_cb_range(500, 50),\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "\n",
    "## plot wind vectors\n",
    "# first, get downsampled wind (so that plot isn't overcrowded)\n",
    "downsample = lambda x, n: x.sel(\n",
    "    latitude=slice(None, None, n), longitude=slice(None, None, n)\n",
    ")\n",
    "comp_downsampled = downsample(comp, n=3)\n",
    "\n",
    "# get grid for plotting\n",
    "xx, yy = np.meshgrid(comp_downsampled.longitude, comp_downsampled.latitude)\n",
    "\n",
    "# finally, plot the vectors\n",
    "qv = ax0.quiver(xx, yy, comp_downsampled[\"uwnd\"], comp_downsampled[\"vwnd\"])\n",
    "\n",
    "## add label for quiver plot\n",
    "ax0.quiverkey(qv, X=1.07, Y=-0.28, U=2, label=r\"2 $m/s$\")\n",
    "\n",
    "## add second plotting background, and remove left longitude labels\n",
    "ax1 = fig.add_subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "ax1, gl1 = plot_setup_helper(ax1, scale=1.2)\n",
    "gl1.left_labels = False\n",
    "\n",
    "## for SLP: plot composite in colors and climatology in contours\n",
    "slp_plot = ax1.contourf(\n",
    "    comp.longitude,\n",
    "    comp.latitude,\n",
    "    comp[\"slp\"],\n",
    "    cmap=\"cmo.balance\",\n",
    "    levels=make_cb_range(500, 50),\n",
    ")\n",
    "ax1.contour(\n",
    "    clim.longitude,\n",
    "    clim.latitude,\n",
    "    clim[\"slp\"],\n",
    "    colors=\"k\",\n",
    "    levels=np.arange(99600, 102400, 400),\n",
    "    linewidths=0.8,\n",
    ")\n",
    "\n",
    "## add colorbars\n",
    "cp_precip = fig.colorbar(\n",
    "    precip_plot,\n",
    "    pad=0.05,\n",
    "    ax=ax0,\n",
    "    orientation=\"horizontal\",\n",
    "    label=r\"$\\Delta$ Precip. (mm/month)\",\n",
    "    ticks=np.arange(-28, 42, 14),\n",
    ")\n",
    "cp_slp = fig.colorbar(\n",
    "    slp_plot,\n",
    "    ax=ax1,\n",
    "    pad=0.05,\n",
    "    orientation=\"horizontal\",\n",
    "    label=r\"$\\Delta$ SLP (Pa)\",\n",
    "    ticks=np.arange(-400, 600, 200),\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e15d4f-6f95-4cf6-8134-4fa3db9d87b9",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06f1a7-be80-4bd4-a15c-a0f832362f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trim to Azores region\n",
    "# slp_noaa_azores =\n",
    "\n",
    "# ## now subset in space and get DJF average (loads into memory)\n",
    "# print(\"Loading LME\")\n",
    "# slp_lme = reduce(slp_lme)\n",
    "\n",
    "# print(\"Loading NOAA\")\n",
    "# slp_noaa = reduce(slp_noaa)\n",
    "\n",
    "# print(\"Loading 20CR\")\n",
    "# slp_era = reduce(slp_era)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6f12b-3dff-4c19-94e7-fea0312a5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cos_lat = np.cos(slp_noaa.latitude * 2 * np.pi / 360)\n",
    "slp_noaa_weighted = slp_noaa.weighted(weights=cos_lat)\n",
    "slp_weighted = slp_noaa_weighted.mean([\"longitude\", \"latitude\"])\n",
    "slp_unweighted = slp_noaa.mean([\"longitude\", \"latitude\"])\n",
    "# (slp_noaa * cos_lat).sum(\"latitude\") / cos_lat.sum(\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15222b7e-d4e5-431e-84a3-e07a4e2e44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_azores = trim_to_azores(slp_noaa)\n",
    "slp_azores = slp_azores.weighted(np.cos(slp_azores.latitude * 2 * np.pi / 360))\n",
    "slp_azores = slp_azores.mean([\"longitude\", \"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cccac-3922-404e-a708-049bb8a87052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "y0 = 1900\n",
    "scipy.stats.pearsonr(\n",
    "    slp_azores.sel(year=slice(y0, None)), slp_weighted.sel(year=slice(y0, None))\n",
    ")\n",
    "\n",
    "scipy.stats.pearsonr(\n",
    "    slp_azores.sel(year=slice(y0, None)), slp_unweighted.sel(year=slice(y0, None))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c990-9a5e-4f24-89bb-7e43a133668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(slp_weighted.year, slp_weighted, label=\"weighted\")\n",
    "ax.plot(slp_weighted.year, spatial_avg(slp_noaa), label=\"weighted\", ls=\"--\", c=\"k\")\n",
    "ax.plot(slp_unweighted.year, slp_unweighted, label=\"unweighted\")\n",
    "ax.plot(slp_azores.year, slp_azores, label=\"azores\")\n",
    "ax.axhline(1.013e5, ls=\"--\", c=\"k\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(slp_azores.year, slp_azores, label=\"azores\")\n",
    "ax.plot(\n",
    "    slp_azores.year, slp_azores - slp_weighted + slp_weighted.mean(), label=\"azores\"\n",
    ")\n",
    "ax.axhline(1.013e5, ls=\"--\", c=\"k\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b14fad-a724-4a75-83c2-95b0f2e8ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy.stats.pearsonr(slp_azores / slp_weighted, slp_azores - slp_weighted))\n",
    "print(\n",
    "    scipy.stats.pearsonr(\n",
    "        np.log(slp_azores) - np.log(slp_weighted), slp_azores - slp_weighted\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e19d7-cca6-4ed5-b463-a694d862687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = lambda x: (x - x.mean()) / x.std()\n",
    "plt.plot(norm(slp_azores / slp_weighted))\n",
    "plt.plot(norm(slp_azores - slp_weighted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a330e9-cced-4091-8a1e-1d2d72f4cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anom(data):\n",
    "    \"\"\"Compute anomalies relative to long-term mean\"\"\"\n",
    "    return\n",
    "\n",
    "\n",
    "def AHA(data_anom):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a64e26-ddba-45e0-be67-5648ff84c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slp_noaa.chunk({\"time\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6603f3-d94b-4507-8061-93207a451a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = slp_noaa.isel(latitude=10, longitude=10)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "ax.plot(djf_avg(x))\n",
    "ax.plot(djf_avg2(x))\n",
    "ax.set_xlim([-1, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899ad98a-b0ef-4671-9e03-27d49ac788b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg2(slp_noaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c115dcd-c38e-4664-b752-10e12687210e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef31b92-0635-42d1-94a7-53df7ac3beb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg2(slp_noaa).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2857f-467f-4b08-b1ad-35900ef91e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "djf_avg(slp_noaa).year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff0ac0-89b1-4a5a-afc3-36b2feff3aaa",
   "metadata": {},
   "source": [
    "# Compute AHA index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d778e6c-87b4-483a-80e4-2ade90e2fa87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
